{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d451bf3",
   "metadata": {},
   "source": [
    "# Proceso a seguir\n",
    "\n",
    "1. Importar datos de entrenamiento\n",
    "1. Aumentar para incluir negativos\n",
    "3. Split train val\n",
    "4. Preprocesamiento de train\n",
    "5. Entrenar modelo con datos de train\n",
    "5. Predicciones con val\n",
    "6. Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e88a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7289 entries, 0 to 7288\n",
      "Data columns (total 18 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   purchase_id             7289 non-null   int64  \n",
      " 1   customer_id             7289 non-null   object \n",
      " 2   customer_date_of_birth  7289 non-null   object \n",
      " 3   customer_gender         5738 non-null   object \n",
      " 4   customer_signup_date    7289 non-null   object \n",
      " 5   item_id                 7289 non-null   object \n",
      " 6   item_title              7289 non-null   object \n",
      " 7   item_category           7289 non-null   object \n",
      " 8   item_price              7289 non-null   float64\n",
      " 9   item_img_filename       7289 non-null   object \n",
      " 10  item_avg_rating         7244 non-null   float64\n",
      " 11  item_num_ratings        7289 non-null   int64  \n",
      " 12  item_release_date       7289 non-null   object \n",
      " 13  purchase_timestamp      7289 non-null   object \n",
      " 14  customer_item_views     7289 non-null   int64  \n",
      " 15  purchase_item_rating    1544 non-null   float64\n",
      " 16  purchase_device         7289 non-null   object \n",
      " 17  label                   7289 non-null   int64  \n",
      "dtypes: float64(3), int64(4), object(11)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "train_file = \"../../datasets/customer_purchases/customer_purchases_train.csv\"\n",
    "train_file = os.path.abspath(train_file)\n",
    "train_df = pd.read_csv(train_file)\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b139e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df saved to D:\\ml25_escuelita\\src\\ml25\\P01_customer_purchases\\boilerplate\\data_processing.py\\..\\..\\..\\datasets\\customer_purchases\\customer_features.csv\n",
      "df saved to D:\\ml25_escuelita\\src\\ml25\\P01_customer_purchases\\boilerplate\\data_processing.py\\..\\..\\..\\datasets\\customer_purchases\\processed_train.csv\n"
     ]
    }
   ],
   "source": [
    "from data_processing import *\n",
    "\n",
    "X, y = read_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8d68cf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'label', 'item_season_spring', 'item_season_summer', 'item_season_winter'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mread_test_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ml25_escuelita\\src\\ml25\\P01_customer_purchases\\boilerplate\\data_processing.py:577\u001b[0m, in \u001b[0;36mread_test_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m    553\u001b[0m     test_df = read_csv(\"customer_purchases_test\")\n\u001b[0;32m    554\u001b[0m     print(test_df.columns)\n\u001b[0;32m    557\u001b[0m # def smart_train_val_split(complete_df, frac_train=0.8, random_state=42): # for customer features extraction\n\u001b[0;32m    558\u001b[0m #     '''\n\u001b[0;32m    559\u001b[0m #     DATA IS ASSUMED TO HAVE POSITIVES AND NEGATIVES\n\u001b[0;32m    560\u001b[0m \n\u001b[0;32m    561\u001b[0m #     This custom train-test split is necessary because customer profiles need to be calculated\n\u001b[0;32m    562\u001b[0m #     based on positive train split only to avoid data leakage. However, this approach requires each customer \n\u001b[0;32m    563\u001b[0m #     to contribute 80% of its purchases to the train split and the remaining 20% to validation.\n\u001b[0;32m    564\u001b[0m #     sklearn's train_val_split implementation doesn't offer the granularity to achieve this requirement.\n\u001b[0;32m    565\u001b[0m #     '''\n\u001b[0;32m    566\u001b[0m #     train_parts = []\n\u001b[0;32m    567\u001b[0m #     val_parts = []\n\u001b[0;32m    568\u001b[0m \n\u001b[0;32m    569\u001b[0m #     for cust_id, group in complete_df.groupby(\"customer_id\"):\n\u001b[0;32m    570\u001b[0m #         if len(group) == 1:\n\u001b[0;32m    571\u001b[0m #             # If customer has only one purchase, send to train\n\u001b[0;32m    572\u001b[0m #             train_parts.append(group)\n\u001b[0;32m    573\u001b[0m #             continue\n\u001b[0;32m    574\u001b[0m \n\u001b[0;32m    575\u001b[0m #         train_g, val_g = train_test_split(\n\u001b[0;32m    576\u001b[0m #             group,\n\u001b[1;32m--> 577\u001b[0m #             train_size = frac_train,\n\u001b[0;32m    578\u001b[0m #             random_state=random_state,\n\u001b[0;32m    579\u001b[0m #             shffle=True,\n\u001b[0;32m    580\u001b[0m #         )\n\u001b[0;32m    581\u001b[0m #         train_parts.append(train_g)\n\u001b[0;32m    582\u001b[0m #         val_parts.append(val_g)\n\u001b[0;32m    583\u001b[0m     \n\u001b[0;32m    584\u001b[0m #     # read train data\n\u001b[0;32m    585\u001b[0m #     # calculate negatives for train_data\n\u001b[0;32m    586\u001b[0m #     # split \n\u001b[0;32m    587\u001b[0m #     #   train: 80% positives + 80% negatives per customer\n\u001b[0;32m    588\u001b[0m #     #   val: 20% positives + 20% negatives per customer\n\u001b[0;32m    589\u001b[0m #     # calculate customer_features on positive train\n\u001b[0;32m    590\u001b[0m #     # merge customer_features on train & val\n",
      "File \u001b[1;32md:\\ml25_escuelita\\src\\ml25\\P01_customer_purchases\\boilerplate\\data_processing.py:300\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(raw_df, training)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03mAgrega tu procesamiento de datos, considera si necesitas guardar valores de entrenamiento.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03mUtiliza la bandera para distinguir entre preprocesamiento de entrenamiento y validación/prueba\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# customer features se debe calcular en base al segmento de entrenamiento\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m#customer_features = extract_customer_features(raw_df)\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m#merged_train_df = merge_customer_profiles(raw_df, customer_features)\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m processed_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# select desired columns to keep and in desired order\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n",
      "File \u001b[1;32md:\\ml25_escuelita\\src\\ml25\\P01_customer_purchases\\boilerplate\\data_processing.py:278\u001b[0m, in \u001b[0;36mprocess_df\u001b[1;34m(df, training)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     preprocessor \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(savepath) \n\u001b[1;32m--> 278\u001b[0m     processed_array \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# --- Build final DataFrame ---\u001b[39;00m\n\u001b[0;32m    281\u001b[0m cat_features \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mnamed_transformers_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget_feature_names_out(categorical_cols)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\joshi\\scoop\\persist\\miniconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\joshi\\scoop\\persist\\miniconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1085\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     diff \u001b[38;5;241m=\u001b[39m all_names \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m-> 1085\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1087\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m     _check_n_features(\u001b[38;5;28mself\u001b[39m, X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: columns are missing: {'label', 'item_season_spring', 'item_season_summer', 'item_season_winter'}"
     ]
    }
   ],
   "source": [
    "test_data = read_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67304a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = read_csv(\"customer_purchases_test\")\n",
    "test_df.info()\n",
    "customer_feat = read_csv(\"customer_features\")\n",
    "#test_df = pd.merge(test_df, customer_feat, on=\"customer_id\")\n",
    "# agregar features derivados del cliente al dataset\n",
    "merged = pd.merge(test_df, customer_feat, on=\"customer_id\", how=\"left\")\n",
    "# Procesamiento de datos\n",
    "processed = preprocess(merged, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faecf5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "\n",
    "def tune_models(X, y):\n",
    "    \"\"\"\n",
    "    Tune SVM and XGBoost using GridSearchCV with ROC-AUC as scoring.\n",
    "    \n",
    "    Prints best hyperparameters for each model.\n",
    "    \n",
    "    Args:\n",
    "        X: feature DataFrame or array\n",
    "        y: labels Series or array\n",
    "    \n",
    "    Returns:\n",
    "        best_estimators: dict with 'svm' and 'xgboost' best estimators\n",
    "        best_scores: dict with best ROC-AUC scores for each\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------- SVM ----------\n",
    "    svm = SVC(probability=True)\n",
    "    svm_param_grid = {\n",
    "        \"C\": [0.1, 1.0, 10],\n",
    "        \"kernel\": [\"rbf\", \"poly\"],\n",
    "        \"gamma\": [\"scale\", \"auto\"]\n",
    "    }\n",
    "    \n",
    "    svm_grid = GridSearchCV(\n",
    "        svm,\n",
    "        svm_param_grid,\n",
    "        scoring=make_scorer(roc_auc_score, needs_proba=True),\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    print(\"Tuning SVM...\")\n",
    "    svm_grid.fit(X, y)\n",
    "    best_svm = svm_grid.best_estimator_\n",
    "    best_svm_score = svm_grid.best_score_\n",
    "    print(\"Best SVM Hyperparameters:\", svm_grid.best_params_)\n",
    "    print(\"Best SVM ROC-AUC:\", best_svm_score)\n",
    "    \n",
    "    # ---------- XGBoost ----------\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_param_grid = {\n",
    "        \"n_estimators\": [100, 300, 500],\n",
    "        \"max_depth\": [3, 6, 9],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"subsample\": [0.7, 1.0],\n",
    "        \"colsample_bytree\": [0.7, 1.0]\n",
    "    }\n",
    "    \n",
    "    xgb_grid = GridSearchCV(\n",
    "        xgb,\n",
    "        xgb_param_grid,\n",
    "        scoring=make_scorer(roc_auc_score, needs_proba=True),\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    print(\"Tuning XGBoost...\")\n",
    "    xgb_grid.fit(X, y)\n",
    "    best_xgb = xgb_grid.best_estimator_\n",
    "    best_xgb_score = xgb_grid.best_score_\n",
    "    print(\"Best XGBoost Hyperparameters:\", xgb_grid.best_params_)\n",
    "    print(\"Best XGBoost ROC-AUC:\", best_xgb_score)\n",
    "    \n",
    "    best_estimators = {\"svm\": best_svm, \"xgboost\": best_xgb}\n",
    "    best_scores = {\"svm\": best_svm_score, \"xgboost\": best_xgb_score}\n",
    "    \n",
    "    return best_estimators, best_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15527351",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = read_train_data()\n",
    "best_models, best_scores = tune_models(X, y)\n",
    "\n",
    "# Access best estimators if needed\n",
    "svm_model = best_models[\"svm\"]\n",
    "xgb_model = best_models[\"xgboost\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
